{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameter optimization in assignment 4(a), 4(b) can be \n",
    "conducted here.\n",
    "Be sure to leave only your best hyperparameter combination\n",
    "here and comment the original hyperparameter settings.\n",
    "\"\"\"\n",
    "\n",
    "# Setting some hyperparameters\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 50\n",
    "INITIAL_LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "REG = 5e-4\n",
    "EPOCHS = 30\n",
    "DATAROOT = \"./data\"\n",
    "CHECKPOINT_PATH = \"./saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train) #训练数据集\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)   #生成一个个batch进行批训练，组成batch的时候顺序打乱取\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\cifar10_trainval.tar.gz\n",
      "Extracting ./data\\cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True,\n",
    "                                          num_workers=1)\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=False, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU...\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = ResNet(ResidualBlock)\n",
    "net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint: ./saved_model/model.h5\n",
      "Starting from epoch 4 \n",
      "Starting from learning rate 0.100000:\n"
     ]
    }
   ],
   "source": [
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model/model.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assignment 2(c)\n",
    "In the targeted classification task, we use cross entropy loss with L2 \n",
    "regularization as the learning object.\n",
    "You need to formulate the cross-entropy loss function in PyTorch.\n",
    "You should also specify a PyTorch Optimizer to optimize this loss function.\n",
    "We recommend you to use the SGD-momentum with an initial learning rate 0.01 \n",
    "and momentum 0.9 as a start.\n",
    "\"\"\"\n",
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(),lr=INITIAL_LR,momentum=MOMENTUM,weight_decay=REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-01 17:42:04.305836\n",
      "Epoch 4:\n",
      "352\n",
      "Training loss: 0.5239, Training accuracy: 0.8169\n",
      "2019-10-01 17:43:35.089755\n",
      "Testing...\n",
      "Testing loss: 0.6735, Testing accuracy: 0.7658\n",
      "Saving ...\n",
      "2019-10-01 17:43:39.437652\n",
      "Epoch 5:\n",
      "352\n",
      "Training loss: 0.4810, Training accuracy: 0.8326\n",
      "2019-10-01 17:45:13.637821\n",
      "Testing...\n",
      "Testing loss: 0.6610, Testing accuracy: 0.7704\n",
      "Saving ...\n",
      "2019-10-01 17:45:18.067819\n",
      "Epoch 6:\n",
      "352\n",
      "Training loss: 0.4440, Training accuracy: 0.8461\n",
      "2019-10-01 17:46:53.181261\n",
      "Testing...\n",
      "Testing loss: 0.5628, Testing accuracy: 0.8000\n",
      "Saving ...\n",
      "2019-10-01 17:46:57.549379\n",
      "Epoch 7:\n",
      "352\n",
      "Training loss: 0.4157, Training accuracy: 0.8556\n",
      "2019-10-01 17:48:32.772515\n",
      "Testing...\n",
      "Testing loss: 0.6025, Testing accuracy: 0.7980\n",
      "2019-10-01 17:48:37.067541\n",
      "Epoch 8:\n",
      "352\n",
      "Training loss: 0.3844, Training accuracy: 0.8658\n",
      "2019-10-01 17:50:12.503019\n",
      "Testing...\n",
      "Testing loss: 0.6529, Testing accuracy: 0.7832\n",
      "2019-10-01 17:50:16.791996\n",
      "Epoch 9:\n",
      "352\n",
      "Training loss: 0.3691, Training accuracy: 0.8722\n",
      "2019-10-01 17:51:52.439291\n",
      "Testing...\n",
      "Testing loss: 0.5939, Testing accuracy: 0.8008\n",
      "Saving ...\n",
      "2019-10-01 17:51:56.790142\n",
      "Epoch 10:\n",
      "352\n",
      "Training loss: 0.3433, Training accuracy: 0.8800\n",
      "2019-10-01 17:53:32.414773\n",
      "Testing...\n",
      "Testing loss: 0.7413, Testing accuracy: 0.7594\n",
      "Current learning rate has decayed to 0.010000\n",
      "2019-10-01 17:53:36.760936\n",
      "Epoch 11:\n",
      "352\n",
      "Training loss: 0.1854, Training accuracy: 0.9408\n",
      "2019-10-01 17:55:12.531767\n",
      "Testing...\n",
      "Testing loss: 0.3687, Testing accuracy: 0.8776\n",
      "Saving ...\n",
      "2019-10-01 17:55:16.944018\n",
      "Epoch 12:\n",
      "352\n",
      "Training loss: 0.1180, Training accuracy: 0.9649\n",
      "2019-10-01 17:56:52.599942\n",
      "Testing...\n",
      "Testing loss: 0.3615, Testing accuracy: 0.8834\n",
      "Saving ...\n",
      "2019-10-01 17:56:57.004975\n",
      "Epoch 13:\n",
      "352\n",
      "Training loss: 0.0832, Training accuracy: 0.9752\n",
      "2019-10-01 17:58:32.652192\n",
      "Testing...\n",
      "Testing loss: 0.3753, Testing accuracy: 0.8830\n",
      "2019-10-01 17:58:36.982064\n",
      "Epoch 14:\n",
      "352\n",
      "Training loss: 0.0587, Training accuracy: 0.9844\n",
      "2019-10-01 18:00:12.693174\n",
      "Testing...\n",
      "Testing loss: 0.3879, Testing accuracy: 0.8824\n",
      "2019-10-01 18:00:16.966181\n",
      "Epoch 15:\n",
      "352\n",
      "Training loss: 0.0422, Training accuracy: 0.9902\n",
      "2019-10-01 18:01:52.680271\n",
      "Testing...\n",
      "Testing loss: 0.3904, Testing accuracy: 0.8826\n",
      "2019-10-01 18:01:57.055795\n",
      "Epoch 16:\n",
      "352\n",
      "Training loss: 0.0298, Training accuracy: 0.9937\n",
      "2019-10-01 18:03:56.921268\n",
      "Testing...\n",
      "Testing loss: 0.4038, Testing accuracy: 0.8854\n",
      "Saving ...\n",
      "2019-10-01 18:04:01.172103\n",
      "Epoch 17:\n",
      "352\n",
      "Training loss: 0.0219, Training accuracy: 0.9961\n",
      "2019-10-01 18:05:35.343531\n",
      "Testing...\n",
      "Testing loss: 0.4147, Testing accuracy: 0.8860\n",
      "Saving ...\n",
      "2019-10-01 18:05:39.603540\n",
      "Epoch 18:\n",
      "352\n",
      "Training loss: 0.0145, Training accuracy: 0.9978\n",
      "2019-10-01 18:07:13.735525\n",
      "Testing...\n",
      "Testing loss: 0.4105, Testing accuracy: 0.8858\n",
      "2019-10-01 18:07:18.109792\n",
      "Epoch 19:\n",
      "352\n",
      "Training loss: 0.0113, Training accuracy: 0.9988\n",
      "2019-10-01 18:08:52.149456\n",
      "Testing...\n",
      "Testing loss: 0.4253, Testing accuracy: 0.8856\n",
      "2019-10-01 18:08:56.422165\n",
      "Epoch 20:\n",
      "352\n",
      "Training loss: 0.0086, Training accuracy: 0.9993\n",
      "2019-10-01 18:10:30.908597\n",
      "Testing...\n",
      "Testing loss: 0.4306, Testing accuracy: 0.8846\n",
      "Current learning rate has decayed to 0.001000\n",
      "2019-10-01 18:10:35.290804\n",
      "Epoch 21:\n",
      "352\n",
      "Training loss: 0.0068, Training accuracy: 0.9997\n",
      "2019-10-01 18:12:09.286593\n",
      "Testing...\n",
      "Testing loss: 0.4242, Testing accuracy: 0.8844\n",
      "2019-10-01 18:12:13.518033\n",
      "Epoch 22:\n",
      "352\n",
      "Training loss: 0.0064, Training accuracy: 0.9998\n",
      "2019-10-01 18:13:47.236903\n",
      "Testing...\n",
      "Testing loss: 0.4244, Testing accuracy: 0.8860\n",
      "2019-10-01 18:13:51.454607\n",
      "Epoch 23:\n",
      "352\n",
      "Training loss: 0.0059, Training accuracy: 0.9998\n",
      "2019-10-01 18:15:25.626561\n",
      "Testing...\n",
      "Testing loss: 0.4214, Testing accuracy: 0.8880\n",
      "Saving ...\n",
      "2019-10-01 18:15:29.955365\n",
      "Epoch 24:\n",
      "352\n",
      "Training loss: 0.0055, Training accuracy: 0.9998\n",
      "2019-10-01 18:17:04.018652\n",
      "Testing...\n",
      "Testing loss: 0.4284, Testing accuracy: 0.8874\n",
      "2019-10-01 18:17:08.362287\n",
      "Epoch 25:\n",
      "352\n",
      "Training loss: 0.0054, Training accuracy: 0.9999\n",
      "2019-10-01 18:18:42.381047\n",
      "Testing...\n",
      "Testing loss: 0.4245, Testing accuracy: 0.8866\n",
      "2019-10-01 18:18:46.597872\n",
      "Epoch 26:\n",
      "352\n",
      "Training loss: 0.0053, Training accuracy: 0.9999\n",
      "2019-10-01 18:20:20.628431\n",
      "Testing...\n",
      "Testing loss: 0.4249, Testing accuracy: 0.8870\n",
      "2019-10-01 18:20:24.853951\n",
      "Epoch 27:\n",
      "352\n",
      "Training loss: 0.0052, Training accuracy: 0.9999\n",
      "2019-10-01 18:21:59.163907\n",
      "Testing...\n",
      "Testing loss: 0.4271, Testing accuracy: 0.8874\n",
      "2019-10-01 18:22:03.489028\n",
      "Epoch 28:\n",
      "352\n",
      "Training loss: 0.0051, Training accuracy: 0.9999\n",
      "2019-10-01 18:23:37.458289\n",
      "Testing...\n",
      "Testing loss: 0.4257, Testing accuracy: 0.8860\n",
      "2019-10-01 18:23:41.985609\n",
      "Epoch 29:\n",
      "352\n",
      "Training loss: 0.0048, Training accuracy: 1.0000\n",
      "2019-10-01 18:25:15.929206\n",
      "Testing...\n",
      "Testing loss: 0.4272, Testing accuracy: 0.8866\n",
      "Optimization finished.\n"
     ]
    }
   ],
   "source": [
    "# Implement Augmentation and batch normalization\n",
    "global_step = 0\n",
    "best_test_acc = 0\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # Train the training dataset for 1 epoch.\n",
    "    print(len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,targets)\n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        # Calculate accuracy\n",
    "        total_examples += len(targets)\n",
    "        correct_examples += np.sum((predicted.cpu() - targets.cpu()).numpy() == 0)\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "        pass\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    train_acc_list.append(avg_acc)\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    print(\"Testing...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs,targets)\n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            if batch_idx == 0:\n",
    "                final_pred = predicted.cpu().numpy()\n",
    "            else:\n",
    "                final_pred = np.append(final_pred,predicted.cpu().numpy())\n",
    "            # Calculate accuracy\n",
    "            total_examples += len(targets)\n",
    "            correct_examples +=  np.sum((predicted.cpu() - targets.cpu()).numpy() == 0)\n",
    "            test_loss += loss\n",
    "\n",
    "    avg_loss = test_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    test_acc_list.append(avg_acc)\n",
    "    print(\"Testing loss: %.4f, Testing accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    \n",
    "    \"\"\"\n",
    "    Assignment 4(b)\n",
    "    Learning rate is an important hyperparameter to tune. Specify a \n",
    "    learning rate decay policy and apply it in your training process. \n",
    "    Briefly describe its impact on the learning curveduring your \n",
    "    training process.    \n",
    "    Reference learning rate schedule: \n",
    "    decay 0.98 for every 2 epochs. You may tune this parameter but \n",
    "    minimal gain will be achieved.\n",
    "    Assignment 4(c)\n",
    "    As we can see from above, hyperparameter optimization is critical \n",
    "    to obtain a good performance of DNN models. Try to fine-tune the \n",
    "    model to over 70% accuracy. You may also increase the number of \n",
    "    epochs to up to 100 during the process. Briefly describe what you \n",
    "    have tried to improve the performance of the LeNet-5 model.\n",
    "    \"\"\"\n",
    "    DECAY_EPOCHS = 10\n",
    "    DECAY = 0.1\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        if i == 0:\n",
    "            current_learning_rate = INITIAL_LR\n",
    "        else:\n",
    "            current_learning_rate *= DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            # Assign the learning rate parameter\n",
    "            param_group['lr'] = current_learning_rate\n",
    "            print(\"Current learning rate has decayed to %f\"%current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_test_acc:\n",
    "        best_test_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(EPOCHS-start_epoch),train_acc_list,label='train')\n",
    "plt.plot(range(EPOCHS-start_epoch),test_acc_list,label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = CIFAR10(root=DATAROOT, train=False, download=False, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output from the DNN.\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,targets)\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        if batch_idx == 0:\n",
    "            final_pred = predicted.cpu().numpy()\n",
    "        else:\n",
    "            final_pred = np.append(final_pred,predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pred_df = pd.read_csv('sample_labels.csv')\n",
    "pred_df['Category'] = final_pred\n",
    "pred_df.to_csv('test.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
